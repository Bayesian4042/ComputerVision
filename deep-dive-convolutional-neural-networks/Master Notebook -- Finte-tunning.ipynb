{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Supporting Functions for Data  '''\n",
    "\n",
    "def get_files_in_folder(path, file_extensions, skip_folder, use_subfolder):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for ext in file_extensions:\n",
    "        pattern = os.path.join(path, '*.' + ext)\n",
    "        files += glob.glob(pattern)\n",
    "\n",
    "    # check subfolder\n",
    "    if use_subfolder:\n",
    "        for subfolder in os.listdir(path):\n",
    "            subfolder_path = os.path.join(path, subfolder)\n",
    "            if os.path.isdir(subfolder_path) and not (subfolder in skip_folder):\n",
    "                files += get_files_in_folder(subfolder_path, file_extensions, skip_folder, use_subfolder)\n",
    "    return files\n",
    "\n",
    "def load_images_by_subfolder(root_dir, validation_ratio, skip_folder=[], use_subfolder=False):\n",
    "    file_extensions =  ['jpeg'] # ['jpg', 'jpeg']\n",
    "    return load_by_subfolder(root_dir, file_extensions, validation_ratio, skip_folder, use_subfolder)\n",
    "\n",
    "def load_features_by_subfolder(root_dir, validation_ratio, skip_folder=[], use_subfolder=False, train_dirs = []):\n",
    "    file_extensions =  ['txt']\n",
    "    shared = load_by_subfolder(root_dir, file_extensions, validation_ratio, skip_folder, use_subfolder)\n",
    "    \n",
    "    # ####################\n",
    "    # This will add extra training data, in case more training directories are provided.\n",
    "    # \n",
    "    # Depending on the filename (which should be *md5hash*.txt), \n",
    "    # it will try to make sure, that no validation feature is used for training\n",
    "    # ###################\n",
    "    if train_dirs:\n",
    "        validation_files = [os.path.basename(p) for p in shared['validation_paths']] \n",
    "        for train_dir in train_dirs:\n",
    "            print('=> Adding trainingdata from: {}'.format(train_dir))\n",
    "            train_only = load_by_subfolder(train_dir, file_extensions, 0, skip_folder, use_subfolder)\n",
    "            # Make sure both have the same labels and the same label order\n",
    "            if train_only['labels'] == shared['labels']:\n",
    "                for i in range(train_only['training_count']):\n",
    "                    # Filter features that are already in the validation group\n",
    "                    if os.path.basename(train_only['training_paths'][i]) not in validation_files:\n",
    "                        shared['training_paths'].append(train_only['training_paths'][i])\n",
    "                        shared['training_labels'].append(train_only['training_labels'][i])\n",
    "                        shared['training_count'] += 1\n",
    "\n",
    "        print('=> Final dataset: {} Training, {} Validation'.format(shared['training_count'], shared['validation_count']))\n",
    "        print('')\n",
    "\n",
    "    return shared\n",
    "\n",
    "def load_by_subfolder(root_dir, file_extensions, validation_ratio, skip_folder=[], use_subfolder=False):\n",
    "    \"\"\"\n",
    "    Create a list of labeled data, seperated in training and validation sets.\n",
    "    Will create a new label/class for every sub-directory in the 'root_dir'.\n",
    "\n",
    "    Args:\n",
    "        root_dir: String path to a folder containing subfolders with data.\n",
    "        validation_ratio: How much of the data should go into the validation set\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing an entry for each subfolder/class, \n",
    "        with paths split into training and validation sets.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    training_paths = []\n",
    "    training_labels = []\n",
    "    validation_paths = []\n",
    "    validation_labels = []\n",
    "        \n",
    "    for folder in os.listdir(root_dir):\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        if not os.path.isdir(folder_path) or folder in skip_folder:\n",
    "            continue # skip files and skipped folders\n",
    "\n",
    "        paths = get_files_in_folder(folder_path, file_extensions, skip_folder, use_subfolder)\n",
    "        if not paths:\n",
    "            continue # skip empty directories\n",
    "\n",
    "        total_count = len(paths)\n",
    "        # split the list into traning and validation\n",
    "        label = re.sub(r'[^a-z0-9]+', ' ', folder.lower())\n",
    "        if (validation_ratio > 0):\n",
    "            paths_sub = paths[::validation_ratio]\n",
    "            del paths[::validation_ratio]\n",
    "        else:\n",
    "            paths_sub = []\n",
    "\n",
    "        # print infos\n",
    "        print('=> Label: {} ({}) [Files: {} Total, {} Training, {} Validation]'.format(\n",
    "            label,\n",
    "            len(labels),\n",
    "            total_count,\n",
    "            len(paths),\n",
    "            len(paths_sub)\n",
    "        ))\n",
    "\n",
    "        # add entries to the result\n",
    "        labels.append(label)\n",
    "        label_index = len(labels) - 1\n",
    "        training_paths += paths\n",
    "        training_labels += [label_index] * len(paths)\n",
    "        validation_paths += paths_sub\n",
    "        validation_labels += [label_index] * len(paths_sub)\n",
    "\n",
    "    print('')\n",
    "    return {\n",
    "        'labels': labels,\n",
    "        'training_count': len(training_paths),\n",
    "        'training_paths': training_paths,\n",
    "        'training_labels': training_labels,\n",
    "        'validation_count': len(validation_paths),\n",
    "        'validation_paths': validation_paths,\n",
    "        'validation_labels': validation_labels\n",
    "    }\n",
    "\n",
    "def load_by_file(file, validation_ratio):\n",
    "    \"\"\"\n",
    "    Create a list of labeled data, seperated in training and validation sets.\n",
    "    Reads the given 'file' line by line, where every line has the format *label* *data_path*\n",
    "\n",
    "    Args:\n",
    "        file: File with a list of labels and data path\n",
    "        validation_ratio: How much of the data should go into the validation set\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing an entry for each subfolder/class, \n",
    "        with paths split into training and validation sets.\n",
    "    \"\"\"\n",
    "    # Groupe the paths by label\n",
    "    labeled_paths = {}\n",
    "    f = open(file)\n",
    "    for line in f: \n",
    "        line = line.strip().split(' ')\n",
    "        label = int(line[1].rstrip())\n",
    "        path = line[0].rstrip()\n",
    "        if label in labeled_paths:\n",
    "            labeled_paths[label].append(path)\n",
    "        else:\n",
    "            labeled_paths[label] = [path]\n",
    "\n",
    "    # Seperate them into training and validation sets \n",
    "    labels = []\n",
    "    training_paths = []\n",
    "    training_labels = []\n",
    "    validation_paths = []\n",
    "    validation_labels = []\n",
    "    for label, paths in labeled_paths.items():\n",
    "        print('Data with label \\'%s\\'' %label)\n",
    "        print('=> Found %i entries' %len(paths))\n",
    "\n",
    "        # split the list into traning and validation\n",
    "        if (validation_ratio > 0):\n",
    "            paths_sub = paths[::validation_ratio]\n",
    "            del paths[::validation_ratio]\n",
    "        else:\n",
    "            paths_sub = []\n",
    "\n",
    "        # print infos\n",
    "        print('  => Training: %i' %len(paths))\n",
    "        print('  => Validation %i' %len(paths_sub))\n",
    "\n",
    "        # add entries to the result\n",
    "        labels.append(label)\n",
    "        label_index = len(labels) - 1\n",
    "        training_paths += paths\n",
    "        training_labels += [label_index] * len(paths)\n",
    "        validation_paths += paths_sub\n",
    "        validation_labels += [label_index] * len(paths_sub)\n",
    "\n",
    "    return {\n",
    "        'labels': labels,\n",
    "        'training_count': len(training_paths),\n",
    "        'training_paths': training_paths,\n",
    "        'training_labels': training_labels,\n",
    "        'validation_count': len(validation_paths),\n",
    "        'validation_paths': validation_paths,\n",
    "        'validation_labels': validation_labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  Supporting operations for calculating accuracy and loss'''\n",
    "\n",
    "from tensorflow.data import Iterator\n",
    "\n",
    "\n",
    "def get_validation_ops(scores, true_classes):\n",
    "    \"\"\"Inserts the operations we need to evaluate the accuracy of our results.\n",
    "\n",
    "    Args:\n",
    "        scores: The new final node that produces results\n",
    "        true_classes: The node we feed the true classes in\n",
    "    Returns:\n",
    "        Evaluation operation: defining the accuracy of the model\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        predicted_index = tf.argmax(scores, 1)\n",
    "        true_index = tf.argmax(true_classes, 1)\n",
    "        correct_pred = tf.equal(predicted_index, true_index)\n",
    "        accuracy_op = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    return accuracy_op, correct_pred, predicted_index, true_index\n",
    "\n",
    "\n",
    "def get_train_op(loss, learning_rate, train_vars, use_adam_optimizer=False):\n",
    "    \"\"\"Inserts the training operation\n",
    "    Creates an optimizer and applies gradient descent to the trainable variables\n",
    "    Check: https://www.tensorflow.org/versions/r0.12/api_docs/python/train/optimizers\n",
    "\n",
    "    Args:\n",
    "        loss: the cross entropy mean (scors <> real class)\n",
    "        train_vars: list of all trainable variables\n",
    "    Returns:\n",
    "        Traning/optizing operation\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"train\"):\n",
    "        if use_adam_optimizer:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        else:\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "        train_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "        # --> minimize() = combines calls compute_gradients() and apply_gradients()\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def get_loss_op(scores, true_classes):\n",
    "    \"\"\"Inserts the operations which calculates the loss.\n",
    "\n",
    "    Args:\n",
    "        scores: The final node that produces results\n",
    "        true_classes: The node we feed the true classes in\n",
    "    Returns: loss operation\n",
    "    \"\"\"\n",
    "    # Op for calculating the loss\n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        # sm = tf.nn.softmax(scores)\n",
    "        # total_loss = true_classes * tf.log(sm)\n",
    "        # loss = -(tf.reduce_mean(total_loss))\n",
    "        #\n",
    "        # softmax_cross_entropy_with_logits \n",
    "        # --> calculates the cross entropy between the softmax score (probaility) and hot encoded class expectation (all \"0\" except one \"1\") \n",
    "        # reduce_mean \n",
    "        # --> computes the mean of elements across dimensions of a tensor (cross entropy values here)\n",
    "        #\n",
    "        loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=true_classes))\n",
    "    return loss_op\n",
    "\n",
    "\n",
    "def get_dataset_ops(data_train, data_val, batch_size, train_size, val_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # shuffle the dataset and create batches\n",
    "    if shuffle:\n",
    "        data_train = data_train.shuffle(train_size)\n",
    "        data_val   = data_val.shuffle(val_size)\n",
    "    \n",
    "    data_train = data_train.batch(batch_size)\n",
    "    data_val   = data_val.batch(batch_size)\n",
    "\n",
    "    # create an reinitializable iterator given the dataset structure\n",
    "    iterator = Iterator.from_structure(data_train.output_types, data_train.output_shapes)\n",
    "    next_batch = iterator.get_next()\n",
    "\n",
    "    # Ops for initializing the two different iterators\n",
    "    init_op_train = iterator.make_initializer(data_train)\n",
    "    init_op_val   = iterator.make_initializer(data_val)\n",
    "\n",
    "    return init_op_train, init_op_val, next_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  Utils '''\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from random import randint  \n",
    "from tensorflow.python.ops.nn_ops import softmax\n",
    "    \n",
    "def save_session_to_checkpoint_file(sess, saver, epoch, path):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    checkpoint = os.path.join(path, datetime.now().strftime(\"%m%d_%H%M%S_\") + 'model_epoch' + str(epoch+1) + '.ckpt')\n",
    "    saver.save(sess, checkpoint)\n",
    "    print(\"Model checkpoint saved at {}\".format(checkpoint))\n",
    "\n",
    "def get_misclassified(corr_pred, paths, pred_index, true_index, scores):\n",
    "    \"\"\"\n",
    "    Returns: a list of tupels (path, predicted label, true label)\n",
    "    \"\"\"\n",
    "    misclassified = []\n",
    "    for i, correct in enumerate(corr_pred):\n",
    "        if not correct:\n",
    "            misclassified.append((paths[i], pred_index[i], true_index[i], scores[i]))\n",
    "\n",
    "    return misclassified\n",
    "\n",
    "def print_misclassified(sess, misclassified, labels):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"\")\n",
    "    print(\"=> Misclassified: %i\" %len(misclassified))\n",
    "    print(\"================================================================\")\n",
    "    for (path, pred_index, true_index, score) in misclassified:\n",
    "        smax = sess.run(softmax(score))\n",
    "\n",
    "        print(\"{} | {} ({}) | {}\".format(\n",
    "            path,\n",
    "            labels[pred_index],\n",
    "            labels[true_index],\n",
    "            smax\n",
    "        ))\n",
    "    print(\"================================================================\")\n",
    "\n",
    "def print_output_header(train_count, val_count):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"=> Getting loss and accuracy for:\")\n",
    "    print(\"  => {} training entries\".format(train_count))\n",
    "    print(\"  => {} validation entries\".format(val_count))\n",
    "    print(\"\")\n",
    "    print(\" Ep  |   Time   |   T Loss   |   V Loss   |  T Accu. |  V Accu.\")\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "\n",
    "def print_output_epoch(epoch, train_loss, train_acc, test_loss, test_acc):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"{:4.0f} | {} | {:.8f} | {:.8f} | {:.6f} | {:.6f}\".format(\n",
    "        epoch,\n",
    "        datetime.now().strftime(\"%H:%M:%S\"),\n",
    "        train_loss,\n",
    "        test_loss,\n",
    "        train_acc,\n",
    "        test_acc\n",
    "    ))\n",
    "\n",
    "def run_training(sess, train_op, loss_op, accuracy_op, iterator_op, get_next_batch_op, ph_data, ph_labels, ph_keep_prob, keep_prob, batches):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sess:\n",
    "        loss_op,\n",
    "        train_op:\n",
    "        accuracy_op:\n",
    "        iterator_op:\n",
    "        get_next_batch_op:\n",
    "        ph_data:\n",
    "        ph_labels:\n",
    "        ph_keep_prob:\n",
    "        keep_prob:\n",
    "        batches:\n",
    "    \"\"\"\n",
    "    # Variables to keep track over different batches\n",
    "    acc = 0.\n",
    "    loss = 0.\n",
    "    # use_batch_for_crossvalidation = randint(0, batches - 2)\n",
    "    # -2 -> -1 = we start at index 0 / -1 we don't want to use the last batch, it might be smaller\n",
    "\n",
    "    sess.run(iterator_op)\n",
    "    for batch_step in range(batches):\n",
    "        # Get next batch of data and run the training operation\n",
    "        data_batch, label_batch, _ = sess.run(get_next_batch_op)\n",
    "        _, batch_loss, batch_acc = sess.run(\n",
    "            [train_op, loss_op, accuracy_op],\n",
    "            feed_dict={ph_data: data_batch, ph_labels: label_batch, ph_keep_prob: keep_prob}\n",
    "        )\n",
    "        loss += batch_loss\n",
    "        acc += batch_acc\n",
    "\n",
    "    acc /= batches\n",
    "    loss /= batches\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def run_validation(sess, loss_op, accuracy_op, correct_prediction_op, predicted_index_op, true_index_op, final_op,\n",
    "                   iterator_op, get_next_batch_op, ph_data, ph_labels, ph_keep_prob, batches, return_misclassified = False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sess:\n",
    "        accuracy_op:\n",
    "        predicted_index_op:\n",
    "        iterator_op:\n",
    "        get_next_batch_op:\n",
    "        ph_data:\n",
    "        ph_labels:\n",
    "        ph_keep_prob:\n",
    "        batches:\n",
    "        return_misclassified:\n",
    "        data\n",
    "    \"\"\"\n",
    "    # Variables to keep track over different batches\n",
    "    acc = 0.\n",
    "    loss = 0.\n",
    "    misclassified = []\n",
    "\n",
    "    sess.run(iterator_op)\n",
    "    for _ in range(batches):\n",
    "        img_batch, label_batch, paths = sess.run(get_next_batch_op)\n",
    "        scores, batch_loss, batch_acc, corr_pred, pred_index, true_index = sess.run(\n",
    "            [final_op, loss_op, accuracy_op, correct_prediction_op, predicted_index_op, true_index_op],\n",
    "            feed_dict={ph_data: img_batch, ph_labels: label_batch, ph_keep_prob: 1.}\n",
    "        )\n",
    "        loss += batch_loss\n",
    "        acc += batch_acc\n",
    "        \n",
    "        if return_misclassified:\n",
    "            misclassified += get_misclassified(corr_pred, paths, pred_index, true_index, scores)\n",
    "    \n",
    "    acc /= batches\n",
    "    loss /= batches\n",
    "    return loss, acc, misclassified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Retrainer Function '''\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "\n",
    "class Retrainer(object):\n",
    "    \"\"\"\n",
    "    Retrain (Finetune) a given model on a new set of categories \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_def, data, image_prep=None, write_checkpoints = False):\n",
    "        self.model_def = model_def\n",
    "        self.data = data\n",
    "        self.num_classes = len(data['labels'])\n",
    "        self.image_prep = image_prep if image_prep else model_def.image_prep # overwrite the default model image prep?\n",
    "        self.write_checkpoints = write_checkpoints\n",
    "\n",
    "    @staticmethod\n",
    "    def print_infos(train_vars, restore_vars, learning_rate, batch_size, keep_prob, use_adam):\n",
    "        \"\"\"Print infos about the current run\n",
    "\n",
    "        Args:\n",
    "            restore_vars: \n",
    "            train_vars:\n",
    "            learning_rate:\n",
    "            batch_size:\n",
    "            keep_prob:\n",
    "        \"\"\"\n",
    "        print(\"=> Will Restore:\")\n",
    "        for var in restore_vars:\n",
    "            print(\"  => {}\".format(var))\n",
    "        print(\"=> Will train:\")\n",
    "        for var in train_vars:\n",
    "            print(\"  => {}\".format(var))\n",
    "        print(\"\")\n",
    "        print(\"=> Learningrate: %.4f\" %learning_rate)\n",
    "        print(\"=> Batchsize: %i\" %batch_size)\n",
    "        print(\"=> Dropout: %.4f\" %(1.0 - keep_prob))\n",
    "        print(\"=> Using Adam Optimizer: %r\" %use_adam)\n",
    "        print(\"\")\n",
    "\n",
    "    def parse_data(self, path, label, is_training):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path:\n",
    "            label:\n",
    "            is_training:\n",
    "\n",
    "        Returns:\n",
    "            image: image loaded and preprocesed\n",
    "            label: converted label number into one-hot-encoding (binary)\n",
    "        \"\"\"\n",
    "        # convert label number into one-hot-encoding\n",
    "        one_hot = tf.one_hot(label, self.num_classes)\n",
    "\n",
    "        # load the image\n",
    "        img_file      = tf.read_file(path)\n",
    "        img_decoded   = tf.image.decode_jpeg(img_file, channels=3)\n",
    "        img_processed = self.image_prep.preprocess_image(\n",
    "            image=img_decoded,\n",
    "            output_height=self.model_def.image_size,\n",
    "            output_width=self.model_def.image_size,\n",
    "            is_training=is_training\n",
    "        )\n",
    "        return img_processed, one_hot, path\n",
    "\n",
    "    def parse_train_data(self, path, label):\n",
    "        return self.parse_data(path, label, True)\n",
    "\n",
    "    def parse_validation_data(self, path, label):\n",
    "        return self.parse_data(path, label, False)\n",
    "    \n",
    "    def create_dataset(self, is_training=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            is_training: Define what kind of Dataset should be returned (traning or validation)\n",
    "\n",
    "        Returns: A Tensorflow Dataset with images and their labels. Either for training or validation.\n",
    "        \"\"\"\n",
    "        paths = self.data['training_paths'] if is_training else self.data['validation_paths']\n",
    "        labels = self.data['training_labels'] if is_training else self.data['validation_labels']\n",
    "        dataset = Dataset.from_tensor_slices((\n",
    "            tf.convert_to_tensor(paths, dtype=tf.string),\n",
    "            tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "        ))\n",
    "        # load and preprocess the images\n",
    "        if is_training:\n",
    "            dataset = dataset.map(self.parse_train_data)\n",
    "        else:\n",
    "            dataset = dataset.map(self.parse_validation_data)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    ############################################################################\n",
    "    def run(self, finetune_layers, epochs, learning_rate = 0.01, batch_size = 128, keep_prob = 1.0, memory_usage = 1.0, \n",
    "            device = '/gpu:0', save_ckpt_dir = '', init_ckpt_file = '', use_adam_optimizer=False):\n",
    "        \"\"\"\n",
    "        Run a training on part of the model (retrain/finetune)\n",
    "\n",
    "        Args:\n",
    "            finetune_layers:\n",
    "            epochs:\n",
    "            learning_rate:\n",
    "            batch_size:\n",
    "            keep_prob:\n",
    "            memory_usage:\n",
    "            device:\n",
    "            show_misclassified:\n",
    "            validate_on_each_epoch:\n",
    "            save_ckpt_dir:\n",
    "            init_ckpt_file:\n",
    "        \"\"\"\n",
    "        # create datasets\n",
    "        data_train = self.create_dataset(is_training=True)\n",
    "        data_val = self.create_dataset(is_training=False)\n",
    "\n",
    "        # Get ops to init the dataset iterators and get a next batch\n",
    "        init_train_iterator_op, init_val_iterator_op, get_next_batch_op = ops.get_dataset_ops(\n",
    "            data_train,\n",
    "            data_val,\n",
    "            batch_size,\n",
    "            self.data['training_count'],\n",
    "            self.data['validation_count'],\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # Initialize model and create input placeholders\n",
    "        with tf.device(device):\n",
    "            ph_images = tf.placeholder(tf.float32, [None, self.model_def.image_size, self.model_def.image_size, 3])\n",
    "            ph_labels = tf.placeholder(tf.float32, [None, self.num_classes])\n",
    "            # Could set the first placholder dimension to batch_size, but this wouldn't work with leftover data that not form a whole batch\n",
    "            ph_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "            model = self.model_def(ph_images, keep_prob=ph_keep_prob, num_classes=self.num_classes, retrain_layer=finetune_layers)\n",
    "            final_op = model.get_final_op()\n",
    "        \n",
    "        # Get a list with all trainable variables and print infos for the current run\n",
    "        retrain_vars = model.get_retrain_vars()\n",
    "        restore_vars = model.get_restore_vars()\n",
    "        self.print_infos(retrain_vars, restore_vars, learning_rate, batch_size, keep_prob, use_adam_optimizer)\n",
    "\n",
    "        # Add/Get the different operations to optimize (loss, train and validate)\n",
    "        with tf.device(device):\n",
    "            loss_op = ops.get_loss_op(final_op, ph_labels)\n",
    "            train_op = ops.get_train_op(loss_op, learning_rate, retrain_vars, use_adam_optimizer)\n",
    "            accuracy_op, correct_prediction_op, predicted_index_op, true_index_op = ops.get_validation_ops(final_op, ph_labels)\n",
    "\n",
    "        # Get the number of training/validation steps per epoch to get through all images\n",
    "        batches_per_epoch_train = int(math.ceil(self.data['training_count'] / (batch_size + 0.0)))\n",
    "        batches_per_epoch_val   = int(math.ceil(self.data['validation_count'] / (batch_size + 0.0)))\n",
    "\n",
    "        # Initialize a saver, create a session config and start a session\n",
    "        saver = tf.train.Saver()\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = memory_usage\n",
    "        with tf.Session(config=config) as sess:\n",
    "            # Init all variables \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            # Load the pretrained variables or a saved checkpoint\n",
    "            if init_ckpt_file:\n",
    "                saver.restore(sess, init_ckpt_file)\n",
    "            else: \n",
    "                model.load_initial_weights(sess)\n",
    "            \n",
    "            utils.print_output_header(self.data['training_count'], self.data['validation_count'])\n",
    "            for epoch in range(epochs):\n",
    "                is_last_epoch = True if (epoch+1) == epochs else False\n",
    "                \n",
    "                train_loss, train_acc = utils.run_training(\n",
    "                    sess,\n",
    "                    train_op,\n",
    "                    loss_op,\n",
    "                    accuracy_op,\n",
    "                    init_train_iterator_op,\n",
    "                    get_next_batch_op,\n",
    "                    ph_images,\n",
    "                    ph_labels,\n",
    "                    ph_keep_prob,\n",
    "                    keep_prob,\n",
    "                    batches_per_epoch_train\n",
    "                )\n",
    "\n",
    "                return_misclassified = is_last_epoch\n",
    "                test_loss, test_acc, misclassified = utils.run_validation(\n",
    "                    sess,\n",
    "                    loss_op,\n",
    "                    accuracy_op,\n",
    "                    correct_prediction_op,\n",
    "                    predicted_index_op,\n",
    "                    true_index_op,\n",
    "                    final_op,\n",
    "                    init_val_iterator_op,\n",
    "                    get_next_batch_op,\n",
    "                    ph_images,\n",
    "                    ph_labels,\n",
    "                    ph_keep_prob,\n",
    "                    batches_per_epoch_val,\n",
    "                    return_misclassified\n",
    "                )\n",
    "                \n",
    "                utils.print_output_epoch(epoch + 1, train_loss, train_acc, test_loss, test_acc)\n",
    "                \n",
    "                # show missclassified list on last epoch\n",
    "                if is_last_epoch:\n",
    "                    utils.print_misclassified(sess, misclassified, self.data['labels'])\n",
    "\n",
    "                # save session in a checkpoint file\n",
    "                if self.write_checkpoints or is_last_epoch:\n",
    "                    utils.save_session_to_checkpoint_file(sess, saver, epoch, save_ckpt_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alexnet.alexnet import AlexNet\n",
    "from vgg.vgg import VGG\n",
    "from inception_v3.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' FINTUNNING ... '''\n",
    "\n",
    "\n",
    "# Input params\n",
    "VALIDATION_RATIO = 5 # e.g. 5 -> every 5th element = 1/5 = 0.2 = 20%\n",
    "USE_SUBFOLDER = True\n",
    "SKIP_FOLDER = []\n",
    "\n",
    "# Learning params\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Network params\n",
    "KEEP_PROB = 1.0 # [0.5]\n",
    "FINETUNE_LAYERS = ['fc6', 'fc7', 'fc8']\n",
    "CHECKPOINT_DIR = '../checkpoints'\n",
    "\n",
    "# HARDWARE USAGE\n",
    "DEVICE = '/cpu:0'\n",
    "MEMORY_USAGE = 1.0\n",
    "\n",
    "def finetune(model_def, data, ckpt_dir, write_checkpoint_on_each_epoch, init_from_ckpt, use_adam_optimizer):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_def:\n",
    "        data:\n",
    "        show_misclassified:\n",
    "        validate_on_each_epoch:\n",
    "        ckpt_dir:\n",
    "        write_checkpoint_on_each_epoch:\n",
    "        init_from_ckpt:\n",
    "        use_adam_optimizer:\n",
    "    \"\"\"\n",
    "    trainer = Retrainer(model_def, data, write_checkpoint_on_each_epoch)\n",
    "    trainer.run(\n",
    "        FINETUNE_LAYERS,\n",
    "        NUM_EPOCHS,\n",
    "        LEARNING_RATE,\n",
    "        BATCH_SIZE,\n",
    "        KEEP_PROB,\n",
    "        MEMORY_USAGE,\n",
    "        DEVICE,\n",
    "        ckpt_dir,\n",
    "        init_from_ckpt,\n",
    "        use_adam_optimizer\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main\n",
    "    \"\"\"\n",
    "    # Parse arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '-image_dir',\n",
    "        type=str,\n",
    "        default='',\n",
    "        help='Folder with trainings/validation images'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-image_file',\n",
    "        type=str,\n",
    "        default='',\n",
    "        help='File with a list of trainings/validation images and their labels'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-write_checkpoint_on_each_epoch',\n",
    "        default=False,\n",
    "        help='Write a checkpoint file on each epoch (default is just once at the end',\n",
    "        action='store_true' # whenever this option is set, the arg is set to true\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-init_from_ckpt',\n",
    "        type=str,\n",
    "        default='',\n",
    "        help='Load this checkpoint file to continue training from this point on'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-model',\n",
    "        type=str,\n",
    "        choices=['vgg', 'alex'],\n",
    "        default='alex',\n",
    "        help='Model to be validated. Default is AlexNet (alex)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-use_adam_optimizer',\n",
    "        default=False,\n",
    "        help='Use Adam optimizer instead of GradientDecent',\n",
    "        action='store_true' # whenever this option is set, the arg is set to true\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    if args.image_dir:\n",
    "        \n",
    "        image_dir = args.image_dir\n",
    "    else :\n",
    "        image_dir = 'path to training/validation images'\n",
    "    \n",
    "    if args.image_file:\n",
    "        \n",
    "        image_file = args.image_file\n",
    "    else :\n",
    "        image_file = 'path to image_files'\n",
    "    \n",
    "    if args.write_checkpoint_on_each_epoch:\n",
    "        \n",
    "        write_checkpoint_on_each_epoch = args.write_checkpoint_on_each_epoch\n",
    "    else :\n",
    "        write_checkpoint_on_each_epoch = 'path to write_checkpoint_on_each_epoch'\n",
    "    \n",
    "    if args.init_from_ckpt:\n",
    "        \n",
    "        init_from_ckpt = args.init_from_ckpt\n",
    "    else :\n",
    "        init_from_ckpt = 'path to init_from_ckpt'\n",
    "    \n",
    "    if args.model:\n",
    "        \n",
    "        model_str = args.model\n",
    "    else :\n",
    "        model_str = 'path to model'\n",
    "    \n",
    "    if args.use_adam_optimizer:\n",
    "        \n",
    "        use_adam_optimizer = args.use_adam_optimizer\n",
    "    else :\n",
    "        use_adam_optimizer = 'use_adam_optimizer'\n",
    "   \n",
    "\n",
    "    # Load images\n",
    "    if not image_dir and not image_file:\n",
    "        print('Provide one of the following options to load images \\'-image_file\\' or \\'-image_dir\\'')\n",
    "        return None\n",
    "    elif image_dir: \n",
    "        if not os.path.exists(image_dir):\n",
    "            print('Image root directory \\'%s\\' not found' %image_dir)\n",
    "            return None\n",
    "        else:\n",
    "            data = data_provider.load_images_by_subfolder(image_dir, VALIDATION_RATIO, SKIP_FOLDER, use_subfolder=USE_SUBFOLDER)\n",
    "    else:\n",
    "        if not os.path.exists(image_file):\n",
    "            print('Image file \\'%s\\' not found' %image_file)\n",
    "            return None\n",
    "        else:\n",
    "            data = data_provider.load_by_file(image_file, VALIDATION_RATIO)\n",
    "\n",
    "    # Set a CNN model definition\n",
    "    if model_str == 'vgg':\n",
    "        model_def = VGG\n",
    "    elif model_str == 'vgg_slim':\n",
    "        model_def = VGGslim\n",
    "    elif model_str == 'inc_v3':\n",
    "        model_def = InceptionV3\n",
    "    elif model_str == 'alex': # default\n",
    "        model_def = AlexNet\n",
    "\n",
    "    # Make sure the checkpoint dir exists\n",
    "    ckpt_dir = os.path.join(CHECKPOINT_DIR, model_str)\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    # Start retraining/finetuning\n",
    "    finetune(model_def, data, ckpt_dir, write_checkpoint_on_each_epoch, init_from_ckpt, use_adam_optimizer)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tflearn]",
   "language": "python",
   "name": "conda-env-tflearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
